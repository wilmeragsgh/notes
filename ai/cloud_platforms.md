---
description: Some concepts and practices about cloud ml providers
---

# Cloud platforms

There are a number of machine learning cloud platforms, we provide _more_ details about _a few_ **below**. In the _next_ few lessons, you will learn how to use **Amazon's SageMaker** to _deploy_ machine learning models. Therefore, we _focused_ on providing _more_ information on [Amazon's SageMaker](https://aws.amazon.com/sagemaker/). To allow for a comparison of features offered by [SageMaker](https://aws.amazon.com/sagemaker/), we also provide detailed information about [Google's ML Engine](https://cloud.google.com/ml-engine/) because it's _most_ similar to **SageMaker**.

## Amazon Web Services \(AWS\)

[Amazon Web Services \(AWS\) SageMaker](https://aws.amazon.com/sagemaker/) is Amazon's cloud service that allows you to _build_, _train_, and _deploy_ machine learning models. Some advantages to using Amazon's SageMaker service are the following:

* Flexibility in Machine Learning Software: SageMaker has the flexibility to enable the use of _any_ programming language or software framework for building, training, and deploying machine learning models in AWS. For the details see the three methods of modeling within SageMaker below
  * [Built-in Algorithms](https://docs.aws.amazon.com/sagemaker/latest/dg/algos.html) - There are at least fifteen built-in algorithms that are easily used within SageMaker. Specifically, built-in algorithms for discrete classification or quantitative analysis using [linear learner](https://docs.aws.amazon.com/sagemaker/latest/dg/linear-learner.html) or [XGBoost](https://docs.aws.amazon.com/sagemaker/latest/dg/xgboost.html), item recommendations using [factorization machine](https://docs.aws.amazon.com/sagemaker/latest/dg/fact-machines.html), grouping based upon attributes using [K-Means](https://docs.aws.amazon.com/sagemaker/latest/dg/k-means.html), an algorithm for [image classification](https://docs.aws.amazon.com/sagemaker/latest/dg/image-classification.html), and many other algorithms.
  * **Custom Algorithms** - There are different programming languages and software frameworks that can be used to develop custom algorithms which include: [PyTorch](https://docs.aws.amazon.com/sagemaker/latest/dg/pytorch.html), [TensorFlow](https://docs.aws.amazon.com/sagemaker/latest/dg/tf.html), [Apache MXNet](https://docs.aws.amazon.com/sagemaker/latest/dg/mxnet.html), [Apache Spark](https://docs.aws.amazon.com/sagemaker/latest/dg/apache-spark.html), and [Chainer](https://docs.aws.amazon.com/sagemaker/latest/dg/chainer.html).
  * [Your Own Algorithms](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms.html) - Regardless of the programming language or software framework, you can use your own algorithm when it **isn't** included within the _built-in_ or _custom algorithms_ **above**.
* **Ability to Explore and Process Data within SageMaker**: [SageMaker](https://aws.amazon.com/sagemaker/) enables the use of [Jupyter Notebooks](https://docs.aws.amazon.com/sagemaker/latest/dg/nbi.html) to explore and process data, along with creation, training, validation, testing, and deployment of machine learning models. This notebook interface makes data exploration and documentation easier.
* Flexibility in Modeling and Deployment: SageMaker provides a number of features and automated tools that make _modeling_ and _deployment_ easier. For the details on these features within SageMaker see below:
  * [Automatic Model Tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html): [SageMaker](https://aws.amazon.com/sagemaker/) provides a feature that allows hyperparameter tuning to find the **best** version of the model for _built-in_ and _custom algorithms_. For built-in algorithms **SageMaker** also provides evaluation metrics to evaluate the performance of your models.
  * [Monitoring Models](https://docs.aws.amazon.com/sagemaker/latest/dg/monitoring-overview.html): [SageMaker](https://aws.amazon.com/sagemaker/) provides features that allow you to monitor your _deployed_ models. Additionally with _model deployment_, one can choose _how much_ traffic to route to _each_ deployed model \(model variant\). More information on routing traffic to model variants can be found [here](https://docs.aws.amazon.com/sagemaker/latest/dg/API_ProductionVariant.html) and [here](https://docs.aws.amazon.com/sagemaker/latest/dg/API_CreateEndpointConfig.html) .
  * **Type of Predictions**: [SageMaker](https://aws.amazon.com/sagemaker/) by _default_ allows for [On-demand](https://docs.aws.amazon.com/sagemaker/latest/dg/ex1-test-model.html) type of predictions where _each_ prediction _request_ can contain _one_ to _many_ requests. **SageMaker** also allows for [Batch](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-batch.html) predictions, and request _data_ size limits are based upon S3 object size limits.

## Google Cloud Platform \(GCP\)

[Google Cloud Platform \(GCP\) ML Engine](https://cloud.google.com/ml-engine/) is Google's cloud service that allows you to _build_, _train_, and _deploy_ machine learning models. Below we have highlighted some of the _**similarities\**_ and _**differences\**_ between these two cloud service platforms.

* **Prediction Costs**: The _**primary difference\**_ between the two is how they handle predictions. With _**SageMaker\**_ _predictions_, you must leave resources running to provide predictions. This enables _less_ latency in providing predictions at the _cost_ of paying for running _idle_ services, if there are no \(or few\) prediction requests made while services are running. With _**ML Engine\**_ _predictions_, one has the option to _not_ leave resources running which reduces cost associated with _infrequent_ or _periodic_ requests. Using this has _more_ latency associated with predictions because the resources are in a offline state until they receive a prediction request. The _increased_ latency is associated to bringing resources back online, but one _only_ pays for the time the resources are _in use_. To see more about [ML Engine pricing](https://cloud.google.com/ml-engine/docs/pricing#node-hour) and [SageMaker pricing](https://cloud.google.com/ml-engine/docs/pricing#node-hour).
* **Ability to Explore and Process Data**: Another _**difference\**_ between **ML Engine** and **SageMaker** is the fact that _Jupyter Notebooks_ are not available within **ML Engine**. To use _Jupyter Notebooks_ within **Google's Cloud Platform** \(GCP\), one would use [Datalab](https://cloud.google.com/datalab/docs/). **GCP** separates data exploration, processing, and transformation into other services. Specifically, [Google's Datalab](https://cloud.google.com/datalab/docs/) can be used for data exploration and data processing, [Dataprep](https://cloud.google.com/dataprep/docs/) can be used to explore and transform raw data into clean data for analysis and processing, and [DataFlow](https://cloud.google.com/dataflow/docs/) can be used to deploy batch and streaming data processing pipelines. Noting that **Amazon Web Services** \(AWS\), also have data processing and transformation pipeline services like [AWS Glue](https://aws.amazon.com/glue/) and [AWS Data Pipeline](https://aws.amazon.com/datapipeline/).
* **Machine Learning Software**: The final _**difference\**_ is that [Google's ML Engine](https://cloud.google.com/ml-engine/) has _less_ flexibility in available software frameworks for building, training, and deploying machine learning models in **GCP** as compared to **Amazon's SageMaker**. For the details regarding the two available software frameworks for modeling within **ML Engine** see **below**.
  * [Google's TensorFlow](https://cloud.google.com/ml-engine/docs/tensorflow/) is an open source machine learning framework that was originally developed by the Google Brain team. [TensorFlow](https://www.tensorflow.org/) can be used for creating, training, and deploying machine learning and deep learning models. [Keras](https://keras.io/) is a higher level API written in Python that runs on top of [TensorFlow](https://www.tensorflow.org/), that's easier to use and allows for faster development. GCP provides both [TensorFlow examples](https://cloud.google.com/ml-engine/docs/tensorflow/samples) and a [Keras example](https://cloud.google.com/ml-engine/docs/tensorflow/samples#census-keras).
  * [Google's Scikit-learn](https://cloud.google.com/ml-engine/docs/scikit/) is an open source machine learning framework in Python that was originally developed as a Google Summer of Code project. [Scikit-learn](https://cloud.google.com/ml-engine/docs/scikit/) and an [XGBoost Python package](https://xgboost.readthedocs.io/en/latest/python/index.html) can be used together for creating, training, and deploying machine learning models. In the in [Google's example](https://cloud.google.com/ml-engine/docs/scikit/training-xgboost), [XGBoost](https://xgboost.readthedocs.io/en/latest/python/index.html) is used for modeling and [Scikit-learn](https://cloud.google.com/ml-engine/docs/scikit/) is used for processing the data.
* Flexibility in Modeling and Deployment: Google's ML Engine provides a number of features and automated tools that make _modeling_ and _deployment_ easier, _similar_ to the those provided by Amazon's SageMaker For the details on these features within ML Engine see below:
  * [Automatic Model Tuning](https://cloud.google.com/ml-engine/docs/tensorflow/hyperparameter-tuning-overview): [Google's ML Engine](https://cloud.google.com/ml-engine/) provides a feature that enables hyperparameter tuning to find the **best** version of the model.
  * [Monitoring Models](https://cloud.google.com/ml-engine/docs/tensorflow/monitor-training): [Google's ML Engine](https://cloud.google.com/ml-engine/) provides features that allow you to monitor your models. Additionally [ML Engine](https://cloud.google.com/ml-engine/) provides methods that enable [managing runtime versions](https://cloud.google.com/ml-engine/docs/tensorflow/versioning) and [managing models and jobs](https://cloud.google.com/ml-engine/docs/tensorflow/managing-models-jobs).
  * **Type of Predictions**: [ML Engine](https://cloud.google.com/ml-engine/) allows for [Online](https://cloud.google.com/ml-engine/docs/tensorflow/online-predict)\(_or On-demand_\) type of predictions where _each_ prediction _request_ can contain _one_ to _many_ requests. **ML Engine** also allows for [Batch](https://cloud.google.com/ml-engine/docs/tensorflow/batch-predict) predictions. More information about **ML Engine's** [Online and Batch predictions](https://cloud.google.com/ml-engine/docs/tensorflow/online-vs-batch-prediction).

