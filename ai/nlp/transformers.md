# Transformers

## Tags
#nlp


**Pre-training**

Transformers pre-training is used to learn the next token based on previous tokens or predict a masked token



**Types of architectures**

![image-20211022063003111](img/image-20211022063003111.png)



**Huggingface lib ecosystem**

![image-20211022064532294](img/image-20211022064532294.png)





@Wilmer, when you have little labelled text you might indeed be better off using the embeddings from sentence-transformers with kNN! 

## Resources
- [Illustrated BERT](https://jalammar.github.io/illustrated-bert/)
- [Illustrated transformers](https://jalammar.github.io/illustrated-transformer/)
- [Transformer paper](https://arxiv.org/pdf/1706.03762.pdf)
- [LSTM paper](file:///Users/wilmerags/Downloads/hochreiter1997.pdf)
- [BERT paper](https://arxiv.org/pdf/1810.04805.pdf)

## Related
- [NLP, RNN and representation](https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/)
- [LSTM intuition](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- [[code/python/tensorflow/Transformers]]
- [[use-case-finetunning]]
- [Sentence-BERT](https://arxiv.org/pdf/1908.10084.pdf)
- [RoBERTa](https://arxiv.org/pdf/1907.11692.pdf)
- [BERT-as-a-service](https://github.com/hanxiao/bert-as-service/)
- [MPNet](https://arxiv.org/pdf/2004.09297.pdf)
- [GloVe](https://aclanthology.org/D14-1162.pdf)
- [Universal Sentence Encoder](https://arxiv.org/abs/1803.11175)
- [On the Sentence Embeddings from Pre-trained Language Models](https://arxiv.org/pdf/2011.05864.pdf)
- [Doc2Vec](https://cs.stanford.edu/~quocle/paragraph_vector.pdf)