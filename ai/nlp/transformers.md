# Transformers



**Pre-training**

Transformers pre-training is used to learn the next token based on previous tokens or predict a masked token



**Types of architectures**

![image-20211022063003111](img/image-20211022063003111.png)



**Huggingface lib ecosystem**

![image-20211022064532294](img/image-20211022064532294.png)





@Wilmer, when you have little labelled text you might indeed be better off using the embeddings from sentence-transformers with kNN! 